{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in yfinance folder: ['.gitkeep', 'AAPL.csv', 'ABNB.csv', 'ADBE.csv', 'ADI.csv', 'ADP.csv', 'ADSK.csv', 'AEP.csv', 'AMD.csv', 'AMGN.csv', 'AMZN.csv', 'ANSS.csv', 'ASML.csv', 'AVGO.csv', 'AZN.csv', 'BIIB.csv', 'BKNG.csv', 'BKR.csv', 'CDNS.csv', 'CEG.csv', 'CHTR.csv', 'CMCSA.csv', 'COST.csv', 'CPRT.csv', 'CRWD.csv', 'CSCO.csv', 'CSX.csv', 'CTAS.csv', 'DASH.csv', 'DDOG.csv', 'DLTR.csv', 'DXCM.csv', 'EA.csv', 'ENPH.csv', 'EXC.csv', 'FANG.csv', 'FAST.csv', 'FISV.csv', 'FTNT.csv', 'GEHC.csv', 'GFS.csv', 'GILD.csv', 'GOOG.csv', 'GOOGL.csv', 'HES.csv', 'HON.csv', 'IDXX.csv', 'ILMN.csv', 'INTC.csv', 'INTU.csv', 'ISRG.csv', 'JD.csv', 'KDP.csv', 'KHC.csv', 'KLAC.csv', 'LCID.csv', 'LRCX.csv', 'LULU.csv', 'MAR.csv', 'MCHP.csv', 'MDLZ.csv', 'MELI.csv', 'META.csv', 'MNST.csv', 'MRNA.csv', 'MRVL.csv', 'MSFT.csv', 'MU.csv', 'NFLX.csv', 'NVDA.csv', 'NXPI.csv', 'ODFL.csv', 'ON.csv', 'ORLY.csv', 'PANW.csv', 'PAYX.csv', 'PCAR.csv', 'PDD.csv', 'PEP.csv', 'PYPL.csv', 'QCOM.csv', 'REGN.csv', 'RIVN.csv', 'ROST.csv', 'SBUX.csv', 'SIRI.csv', 'SNPS.csv', 'SWKS.csv', 'TEAM.csv', 'TMUS.csv', 'TSLA.csv', 'TTD.csv', 'TXN.csv', 'VRSK.csv', 'VRSN.csv', 'VRTX.csv', 'WBA.csv', 'WBD.csv', 'WDAY.csv', 'XEL.csv', 'ZM.csv', 'ZS.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pandas_ta as ta  # Technical indicators\n",
    "from tqdm import tqdm   # Progress bars\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "RAW_DIR = \"../data/raw/yfinance\"\n",
    "PROCESSED_DIR = \"../data/processed\"\n",
    "print(\"Files in yfinance folder:\", os.listdir(RAW_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk Load All YFinance Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 28/102 [00:00<00:00, 278.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed FISV.csv: <class 'pandas.core.indexes.base.Index'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:00<00:00, 309.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 36200 rows from 100 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_all_yfinance():\n",
    "    \"\"\"Load and combine multiple YFinance stock data CSVs into one DataFrame.\n",
    "    Handles metadata rows, standardizes columns, and ensures date continuity.\"\"\"\n",
    "    \n",
    "    all_dfs = []  # Will store individual stock DataFrames before concatenation\n",
    "    \n",
    "    # Process each CSV file in directory (with progress bar)\n",
    "    for file in tqdm(os.listdir(RAW_DIR)):\n",
    "        if file.endswith('.csv'):\n",
    "            try:\n",
    "                # Extract ticker symbol from filename (e.g. 'AAPL.csv' -> 'AAPL')\n",
    "                ticker = file.split('.')[0]\n",
    "                \n",
    "                # Read CSV with specific formatting:\n",
    "                # - Skip 3 metadata rows\n",
    "                # - Standardize column names\n",
    "                # - Parse dates as datetime objects\n",
    "                df = pd.read_csv(\n",
    "                    f\"{RAW_DIR}/{file}\",\n",
    "                    skiprows=3,\n",
    "                    names=['date', 'close', 'high', 'low', 'open', 'volume'],\n",
    "                    parse_dates=['date']\n",
    "                )\n",
    "                \n",
    "                # Add ticker column to identify the stock\n",
    "                df['ticker'] = ticker\n",
    "                \n",
    "                # Ensure continuous daily data:\n",
    "                # 1. Set date as index\n",
    "                # 2. Force daily frequency (inserts NaNs for missing dates)\n",
    "                # 3. Forward-fill missing values\n",
    "                # 4. Reset index to return date to a column\n",
    "                df = df.set_index('date').asfreq('D').ffill().reset_index()\n",
    "                \n",
    "                all_dfs.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Skip problematic files but log the error\n",
    "                print(f\"âš ï¸ Failed {file}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    # Combine all DataFrames, ignoring original indices\n",
    "    return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Execute and verify load\n",
    "df = load_all_yfinance()\n",
    "print(f\"âœ… Loaded {len(df)} rows from {df['ticker'].nunique()} stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code verification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date       close        high         low        open      volume  \\\n",
      "0 2024-04-01  169.230911  170.445179  168.683493  170.385463  46240500.0   \n",
      "1 2024-04-02  168.046494  168.544144  167.439360  168.285371  49329500.0   \n",
      "2 2024-04-03  168.852707  169.877865  167.787743  167.996748  47691700.0   \n",
      "3 2024-04-04  168.026611  171.112033  168.026611  169.489689  53704400.0   \n",
      "4 2024-04-05  168.783035  169.589226  168.155991  168.792983  42055200.0   \n",
      "5 2024-04-06  168.783035  169.589226  168.155991  168.792983  42055200.0   \n",
      "6 2024-04-07  168.783035  169.589226  168.155991  168.792983  42055200.0   \n",
      "7 2024-04-08  167.658356  168.404831  167.449351  168.235632  37425500.0   \n",
      "8 2024-04-09  168.872604  169.280681  167.558816  167.907162  42451200.0   \n",
      "9 2024-04-10  166.991486  168.295327  166.324636  168.006696  49709300.0   \n",
      "\n",
      "  ticker  \n",
      "0   AAPL  \n",
      "1   AAPL  \n",
      "2   AAPL  \n",
      "3   AAPL  \n",
      "4   AAPL  \n",
      "5   AAPL  \n",
      "6   AAPL  \n",
      "7   AAPL  \n",
      "8   AAPL  \n",
      "9   AAPL  \n",
      "Tickers: ['AAPL', 'ABNB', 'ADBE', 'ADI', 'ADP']...\n"
     ]
    }
   ],
   "source": [
    "print(df[df['ticker'] == 'AAPL'].head(10))\n",
    "unique_tickers = df['ticker'].unique().tolist()\n",
    "print(f\"Tickers: {unique_tickers[:5]}...\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning Pipeline\n",
    "Goal: Fix common data quality issues\n",
    "\n",
    "Why?\n",
    "Ensures dates are recognized as timestamps (not strings)\n",
    ", Handles market closures without leaving gaps\n",
    ", Guarantees no NaN values break your models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Convert text dates to proper datetime format (essential for time series)\n",
    "    df['date'] = pd.to_datetime(df['date'])  \n",
    "    \n",
    "    # Forward-fill missing values (e.g., weekends/holidays when markets are closed)\n",
    "    df = df.sort_values(['ticker', 'date'])\n",
    "    df = df.groupby('ticker').apply(lambda x: x.ffill())  # Carry last known value forward\n",
    "    \n",
    "    # Remove any remaining bad rows\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN values: 0\n",
      "\n",
      "NaN per column:\n",
      "date      0\n",
      "close     0\n",
      "high      0\n",
      "low       0\n",
      "open      0\n",
      "volume    0\n",
      "ticker    0\n",
      "dtype: int64\n",
      "(36200, 7)\n",
      "        date       close        high         low        open      volume  \\\n",
      "0 2024-04-01  169.230911  170.445179  168.683493  170.385463  46240500.0   \n",
      "1 2024-04-02  168.046494  168.544144  167.439360  168.285371  49329500.0   \n",
      "2 2024-04-03  168.852707  169.877865  167.787743  167.996748  47691700.0   \n",
      "3 2024-04-04  168.026611  171.112033  168.026611  169.489689  53704400.0   \n",
      "4 2024-04-05  168.783035  169.589226  168.155991  168.792983  42055200.0   \n",
      "5 2024-04-06  168.783035  169.589226  168.155991  168.792983  42055200.0   \n",
      "6 2024-04-07  168.783035  169.589226  168.155991  168.792983  42055200.0   \n",
      "7 2024-04-08  167.658356  168.404831  167.449351  168.235632  37425500.0   \n",
      "8 2024-04-09  168.872604  169.280681  167.558816  167.907162  42451200.0   \n",
      "9 2024-04-10  166.991486  168.295327  166.324636  168.006696  49709300.0   \n",
      "\n",
      "  ticker  \n",
      "0   AAPL  \n",
      "1   AAPL  \n",
      "2   AAPL  \n",
      "3   AAPL  \n",
      "4   AAPL  \n",
      "5   AAPL  \n",
      "6   AAPL  \n",
      "7   AAPL  \n",
      "8   AAPL  \n",
      "9   AAPL  \n"
     ]
    }
   ],
   "source": [
    "# Total NaN values in entire DataFrame\n",
    "total_nans = df.isna().sum().sum()\n",
    "print(f\"Total NaN values: {total_nans}\")\n",
    "\n",
    "# NaN count per column\n",
    "nan_per_column = df.isna().sum()\n",
    "print(\"\\nNaN per column:\")\n",
    "print(nan_per_column)\n",
    "print(df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "Goal: Add technical indicators traders use\n",
    "\n",
    "Why These Indicators?\n",
    "\n",
    "SMA: Smooths price noise to reveal trends\n",
    "\n",
    "RSI: Identifies potential reversals (values >70 = overbought, <30 = oversold)\n",
    "\n",
    "MACD: Shows momentum shifts\n",
    "\n",
    "Bollinger Bands: Highlights volatility extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df):\n",
    "    \"\"\"Calculate technical indicators for each stock in the DataFrame.\n",
    "    \n",
    "    Applies common trading indicators to each stock's price series independently\n",
    "    to avoid cross-contamination between different securities.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing stock data with columns: ['ticker', 'date', 'close', ...]\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional technical indicator columns:\n",
    "        - sma_20: 20-day Simple Moving Average (trend identification)\n",
    "        - rsi_14: 14-day Relative Strength Index (momentum)\n",
    "        - macd: MACD line (12,26,9 EMAs crossover system)\n",
    "        - boll_high: Upper Bollinger Band (2 std dev above SMA)\n",
    "        - boll_low: Lower Bollinger Band (2 std dev below SMA)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group by ticker to ensure calculations are performed per-stock\n",
    "    # Using apply(lambda) pattern for clean column-wise operations\n",
    "    return df.groupby('ticker').apply(lambda x: x.assign(\n",
    "        # Trend indicator: 20-period Simple Moving Average\n",
    "        # Helps identify the prevailing market direction\n",
    "        sma_20=ta.sma(x['close'], 20),\n",
    "        \n",
    "        # Momentum oscillator: 14-period RSI\n",
    "        # Measures speed of price movements (range: 0-100)\n",
    "        # >70 = overbought, <30 = oversold\n",
    "        rsi_14=ta.rsi(x['close'], 14),\n",
    "        \n",
    "        # MACD Line (12,26,9 EMA configuration)\n",
    "        # Shows relationship between two moving averages\n",
    "        # Positive = upward momentum, Negative = downward momentum\n",
    "        macd=ta.macd(x['close'])['MACD_12_26_9'],\n",
    "        \n",
    "        # Bollinger Bands (20MA Â± 2 standard deviations)\n",
    "        # Identifies overextended price moves\n",
    "        # Prices near upper band = potentially overbought\n",
    "        # Prices near lower band = potentially oversold\n",
    "        boll_high=ta.bbands(x['close'])['BBU_5_2.0'],\n",
    "        boll_low=ta.bbands(x['close'])['BBL_5_2.0']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New columns: ['date', 'close', 'high', 'low', 'open', 'volume', 'ticker', 'sma_20', 'rsi_14', 'macd', 'boll_high', 'boll_low']\n",
      "\n",
      "AAPL data with indicators:\n",
      "                 date       close        high         low        open  \\\n",
      "ticker                                                                  \n",
      "AAPL   359 2025-03-26  221.529999  225.020004  220.470001  223.509995   \n",
      "       360 2025-03-27  223.850006  224.990005  220.559998  221.389999   \n",
      "       361 2025-03-28  217.899994  223.809998  217.679993  221.669998   \n",
      "\n",
      "                volume ticker      sma_20     rsi_14      macd   boll_high  \\\n",
      "ticker                                                                       \n",
      "AAPL   359  34466100.0   AAPL  220.475502  45.985743 -3.742026  224.668996   \n",
      "       360  37094800.0   AAPL  219.714502  50.019522 -3.144314  225.776406   \n",
      "       361  39784100.0   AAPL  218.656001  41.466587 -3.114834  225.945208   \n",
      "\n",
      "              boll_low  \n",
      "ticker                  \n",
      "AAPL   359  216.351005  \n",
      "       360  217.475596  \n",
      "       361  217.158790  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alaa\\AppData\\Local\\Temp\\ipykernel_23256\\3913432891.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('ticker').apply(lambda x: x.assign(\n"
     ]
    }
   ],
   "source": [
    "df_features = add_technical_indicators(df)\n",
    "# Check new columns\n",
    "print(\"New columns:\", df_features.columns.tolist())\n",
    "# See sample data for AAPL\n",
    "print(\"\\nAAPL data with indicators:\")\n",
    "print(df_features[df_features['ticker'] == 'AAPL'].tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining NaNs: 0\n",
      "                date       close        high         low        open  \\\n",
      "ticker                                                                 \n",
      "AAPL   25 2024-04-26  168.504349  170.534755  168.384902  169.081625   \n",
      "       26 2024-04-27  168.504349  170.534755  168.384902  169.081625   \n",
      "       27 2024-04-28  168.504349  170.534755  168.384902  169.081625   \n",
      "       28 2024-04-29  172.684616  175.202725  172.286502  172.555222   \n",
      "       29 2024-04-30  169.529526  174.167629  169.201075  172.515427   \n",
      "\n",
      "               volume ticker      sma_20     rsi_14      macd   boll_high  \\\n",
      "ticker                                                                      \n",
      "AAPL   25  44838400.0   AAPL  168.864148  50.389421 -1.162209  170.483085   \n",
      "       26  44838400.0   AAPL  168.850214  50.389421 -1.007624  170.140256   \n",
      "       27  44838400.0   AAPL  168.892513  50.389421 -0.875027  169.134174   \n",
      "       28  68169400.0   AAPL  169.083114  63.084213 -0.427700  172.716518   \n",
      "       29  65934800.0   AAPL  169.210016  52.222437 -0.324044  172.783498   \n",
      "\n",
      "             boll_low  \n",
      "ticker                 \n",
      "AAPL   25  164.316042  \n",
      "       26  166.036367  \n",
      "       27  167.997943  \n",
      "       28  166.199174  \n",
      "       29  166.307377  \n"
     ]
    }
   ],
   "source": [
    "# Forward-fill missing values (carry last known observation forward)\n",
    "# NOTE: This maintains continuous time series but WON'T fill:\n",
    "# 1. Leading NaNs (where no prior value exists to fill from)\n",
    "# 2. Technical indicator warm-up periods (e.g., first 19 rows of SMA20)\n",
    "# For complete NaN removal, use .dropna() after this\n",
    "df_features = df_features.ffill()\n",
    "\n",
    "# drop early rows with NaN\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "print(\"Remaining NaNs:\", df_features.isna().sum().sum())  # Should be 0\n",
    "print(df_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any ticker has date gaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max gap between dates per ticker (should be <=3 days for weekends):\n",
      "date\n",
      "1    100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "date_gaps = df.groupby('ticker')['date'].apply(lambda x: x.sort_values().diff().max())\n",
    "print(\"Max gap between dates per ticker (should be <=3 days for weekends):\")\n",
    "print(date_gaps.dt.days.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicate dates per ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_dates = df.groupby(['ticker', 'date']).size().loc[lambda x: x > 1]\n",
    "if not duplicate_dates.empty:\n",
    "    print(\"âš ï¸ Duplicate dates found:\", duplicate_dates.index.tolist())\n",
    "\n",
    "# Verify volume > 0 for all rows\n",
    "zero_volume = df[df['volume'] <= 0]\n",
    "if not zero_volume.empty:\n",
    "    print(\"âš ï¸ Stocks with zero volume:\", zero_volume['ticker'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Data Quality Checks...\n",
      "\n",
      "âš™ï¸ Calculating Features with Safe NaN Handling...\n",
      "\n",
      "ðŸ§¹ Final NaN Cleanup...\n",
      "\n",
      "âœ… Final Validation\n",
      "\n",
      "ðŸŽ‰ Cleaning complete! Final shape: (36175, 15)\n",
      "Available columns: ['date', 'close', 'high', 'low', 'open', 'volume', 'ticker', 'sma_20', 'rsi_14', 'macd', 'boll_high', 'boll_low', 'volume_ma_20', 'volume_spike', 'atr_14']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alaa\\AppData\\Local\\Temp\\ipykernel_23256\\3084778357.py:88: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_features = df_features.groupby('ticker', group_keys=False).apply(safe_technical)\n"
     ]
    }
   ],
   "source": [
    "# Key Tasks:\n",
    "# 1. Resolve data structure issues\n",
    "# 2. Validate data quality\n",
    "# 3. Calculate technical indicators\n",
    "# 4. Handle missing data appropriately\n",
    "\n",
    "# 1. CREATE CLEAN WORKING COPY\n",
    "# Always work on copies to prevent accidental mutation of source data\n",
    "# Note: This breaks reference chains that can cause SettingWithCopy warnings\n",
    "df_features = df_features.copy()\n",
    "\n",
    "# 2. FIX INDEX/COLUMN CONFLICTS\n",
    "# Common issue after groupby operations - ticker ends up in both index and columns\n",
    "if 'ticker' in df_features.index.names:\n",
    "    # Save the index values before resetting\n",
    "    ticker_values = df_features.index.get_level_values('ticker')\n",
    "    \n",
    "    # Clean slate - remove all index levels\n",
    "    # drop=True prevents old index from becoming new column\n",
    "    df_features = df_features.reset_index(drop=True)\n",
    "    \n",
    "    # Only add ticker back if it's not already a column\n",
    "    # Prevents duplicate columns which break pandas operations\n",
    "    if 'ticker' not in df_features.columns:\n",
    "        df_features['ticker'] = ticker_values\n",
    "\n",
    "# 3. DATA QUALITY CHECKS\n",
    "print(\"\\nðŸ” Running Data Quality Checks...\")\n",
    "\n",
    "# Check 1: Negative Prices (impossible in real markets)\n",
    "# Important because some indicators break with negative values\n",
    "negative_prices = df_features[\n",
    "    (df_features['close'] <= 0) | \n",
    "    (df_features['open'] <= 0) |\n",
    "    (df_features['high'] <= 0) |\n",
    "    (df_features['low'] <= 0)\n",
    "]\n",
    "if not negative_prices.empty:\n",
    "    print(\"âš ï¸ Negative prices found in:\", negative_prices['ticker'].unique())\n",
    "    # Remove corrupt rows completely - can't fix bad prices\n",
    "    df_features = df_features[~df_features.index.isin(negative_prices.index)]\n",
    "\n",
    "# Check 2: Price Relationship Sanity\n",
    "# Ensures high > low, high >= close, etc. (market mechanics)\n",
    "invalid_prices = df_features[\n",
    "    (df_features['high'] < df_features['low']) |\n",
    "    (df_features['high'] < df_features['close']) |\n",
    "    (df_features['low'] > df_features['open'])\n",
    "]\n",
    "if not invalid_prices.empty:\n",
    "    print(\"âš ï¸ Illogical prices in:\", invalid_prices['ticker'].unique())\n",
    "    # Drop invalid rows - likely data corruption\n",
    "    df_features = df_features[~df_features.index.isin(invalid_prices.index)]\n",
    "\n",
    "# 4. FEATURE ENGINEERING (WITH NaN PROTECTION)\n",
    "print(\"\\nâš™ï¸ Calculating Features with Safe NaN Handling...\")\n",
    "\n",
    "# Volume Analysis - helps detect unusual activity\n",
    "# Using min_periods=1 means we get values even with limited history\n",
    "df_features['volume_ma_20'] = df_features.groupby('ticker')['volume'].transform(\n",
    "    lambda x: x.rolling(20, min_periods=1).mean()\n",
    ")\n",
    "# Volume spike ratio (safe division)\n",
    "df_features['volume_spike'] = np.where(\n",
    "    df_features['volume_ma_20'] > 0,  # Prevent divide-by-zero\n",
    "    df_features['volume'] / df_features['volume_ma_20'],\n",
    "    1.0  # Neutral value when no history exists\n",
    ")\n",
    "\n",
    "# Technical Indicators Wrapper\n",
    "# Group-level calculation with error handling\n",
    "def safe_technical(group):\n",
    "    \"\"\"Calculate indicators for a single stock with error protection\"\"\"\n",
    "    try:\n",
    "        # Trend indicators\n",
    "        group['sma_20'] = ta.sma(group['close'], length=20)  # 20-day moving average\n",
    "        # Momentum indicators\n",
    "        group['rsi_14'] = ta.rsi(group['close'], length=14)  # Relative Strength Index\n",
    "        # Volatility indicator\n",
    "        group['atr_14'] = ta.atr(group['high'], group['low'], group['close'], length=14)\n",
    "        return group\n",
    "    except Exception as e:\n",
    "        # Log errors but keep processing other stocks\n",
    "        print(f\"âš ï¸ Error calculating indicators for {group.name}: {str(e)}\")\n",
    "        return group\n",
    "\n",
    "# Apply to all stocks in parallel\n",
    "df_features = df_features.groupby('ticker', group_keys=False).apply(safe_technical)\n",
    "\n",
    "# 5. NaN MANAGEMENT STRATEGY\n",
    "print(\"\\nðŸ§¹ Final NaN Cleanup...\")\n",
    "\n",
    "# Technical indicators create NaNs during warm-up periods:\n",
    "# - SMA20: First 19 days\n",
    "# - RSI14: First 13 days  \n",
    "# Strategy: Forward fill within each stock's data\n",
    "tech_cols = ['sma_20', 'rsi_14', 'atr_14']\n",
    "for col in tech_cols:\n",
    "    if col in df_features.columns:\n",
    "        # Fill NaNs with last valid observation per stock\n",
    "        df_features[col] = df_features.groupby('ticker')[col].ffill()\n",
    "\n",
    "# Final NaN treatment - domain-specific fallbacks\n",
    "fill_values = {\n",
    "    'sma_20': df_features['close'],  # If no SMA, use raw price\n",
    "    'rsi_14': 50,                   # Neutral RSI level\n",
    "    'atr_14': 0,                     # Assume no volatility\n",
    "    'volume_spike': 1                # No spike\n",
    "}\n",
    "for col, val in fill_values.items():\n",
    "    if col in df_features.columns:\n",
    "        df_features[col] = df_features[col].fillna(val)\n",
    "\n",
    "# 6. FINAL VALIDATION\n",
    "print(\"\\nâœ… Final Validation\")\n",
    "# Hard checks - fail fast if data isn't clean\n",
    "assert df_features.isna().sum().sum() == 0, (\n",
    "    f\"Final NaN count: {df_features.isna().sum().sum()}\\n\"\n",
    "    f\"NaN columns: {df_features.columns[df_features.isna().any()].tolist()}\"\n",
    ")\n",
    "assert (df_features['close'] > 0).all(), \"Negative prices exist!\"\n",
    "\n",
    "# Ensure chronological order for time series analysis\n",
    "df_features.sort_values(['ticker', 'date'], inplace=True)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Cleaning complete! Final shape: {df_features.shape}\")\n",
    "print(\"Available columns:\", df_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARKET DAY FLAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date       close        high         low        open       volume  \\\n",
      "0 2024-04-26  168.504349  170.534755  168.384902  169.081625   44838400.0   \n",
      "1 2024-04-27  168.504349  170.534755  168.384902  169.081625   44838400.0   \n",
      "2 2024-04-28  168.504349  170.534755  168.384902  169.081625   44838400.0   \n",
      "3 2024-04-29  172.684616  175.202725  172.286502  172.555222   68169400.0   \n",
      "4 2024-04-30  169.529526  174.167629  169.201075  172.515427   65934800.0   \n",
      "5 2024-05-01  168.504349  171.898327  168.315239  168.783032   50383100.0   \n",
      "6 2024-05-02  172.216812  172.604979  170.086870  171.699252   94214900.0   \n",
      "7 2024-05-03  182.518188  186.121171  181.801571  185.772810  163224100.0   \n",
      "8 2024-05-04  182.518188  186.121171  181.801571  185.772810  163224100.0   \n",
      "9 2024-05-05  182.518188  186.121171  181.801571  185.772810  163224100.0   \n",
      "\n",
      "  ticker      sma_20  rsi_14      macd   boll_high    boll_low  volume_ma_20  \\\n",
      "0   AAPL  168.504349    50.0 -1.162209  170.483085  164.316042  4.483840e+07   \n",
      "1   AAPL  168.504349    50.0 -1.007624  170.140256  166.036367  4.483840e+07   \n",
      "2   AAPL  168.504349    50.0 -0.875027  169.134174  167.997943  4.483840e+07   \n",
      "3   AAPL  172.684616    50.0 -0.427700  172.716518  166.199174  5.067115e+07   \n",
      "4   AAPL  169.529526    50.0 -0.324044  172.783498  166.307377  5.372388e+07   \n",
      "5   AAPL  168.504349    50.0 -0.320920  172.783498  166.307377  5.316708e+07   \n",
      "6   AAPL  172.216812    50.0 -0.018665  173.910326  166.665534  5.903106e+07   \n",
      "7   AAPL  182.518188    50.0  1.040120  183.031431  163.149966  7.205519e+07   \n",
      "8   AAPL  182.518188    50.0  1.857800  187.479829  162.634996  8.218507e+07   \n",
      "9   AAPL  182.518188    50.0  2.477260  189.796319  165.513972  9.028897e+07   \n",
      "\n",
      "   volume_spike  atr_14  is_market_open  daily_return  \n",
      "0      1.000000     0.0            True           NaN  \n",
      "1      1.000000     0.0            True      0.000000  \n",
      "2      1.000000     0.0            True      0.000000  \n",
      "3      1.345330     0.0            True      0.024808  \n",
      "4      1.227290     0.0            True     -0.018271  \n",
      "5      0.947637     0.0            True     -0.006047  \n",
      "6      1.596023     0.0            True      0.022032  \n",
      "7      2.265265     0.0            True      0.059816  \n",
      "8      1.986055     0.0            True      0.000000  \n",
      "9      1.807797     0.0            True      0.000000  \n",
      "âœ… Saved cleaned data (3.5MB)\n",
      "Columns persisted: ['date', 'close', 'high', 'low', 'open', 'volume', 'ticker', 'sma_20', 'rsi_14', 'macd', 'boll_high', 'boll_low', 'volume_ma_20', 'volume_spike', 'atr_14', 'is_market_open', 'daily_return']\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# Creates boolean flag for trading vs non-trading days\n",
    "# Critical for: \n",
    "# - Avoiding false signals from weekend/holiday data\n",
    "# - Volume spike analysis\n",
    "# Note: Uses volume > 0 as proxy for market open (more reliable than calendar)\n",
    "df_features['is_market_open'] = df_features['volume'] > 0\n",
    "\n",
    "# 2. PRICE CHANGE ANALYSIS\n",
    "# ------------------------\n",
    "# Calculates daily returns with protection against:\n",
    "# - Division by zero\n",
    "# - Look-ahead bias\n",
    "# Groups by ticker to prevent cross-stock contamination\n",
    "df_features['daily_return'] = df_features.groupby('ticker')['close'].pct_change()\n",
    "\n",
    "# Extreme move detection (50% daily change)\n",
    "# Important for:\n",
    "# - Identifying potential data errors\n",
    "# - Flagging corporate actions (splits, dividends)\n",
    "# - Risk management scenarios\n",
    "extreme_returns = df_features[np.abs(df_features['daily_return']) > 0.5]\n",
    "if not extreme_returns.empty:\n",
    "    # Log but don't fail - some stocks legitimately gap\n",
    "    print(f\"âš ï¸ {len(extreme_returns)} extreme returns (>50%) detected\")\n",
    "    print(\"Sample affected tickers:\", extreme_returns['ticker'].unique()[:5])\n",
    "    \n",
    "\n",
    "\n",
    "print(df_features.head(10))\n",
    "\n",
    "# 3. DATA PERSISTENCE\n",
    "# -------------------\n",
    "# Saves cleaned data in Parquet format because:\n",
    "# - Preserves dtypes (no floatâ†’string conversion like CSV)\n",
    "# - Efficient columnar storage (faster for time series)\n",
    "# - Built-in compression (gzip offers good balance)\n",
    "# - Maintains schema on read\n",
    "df_features.to_parquet(\n",
    "    f\"{PROCESSED_DIR}/cleaned_stocks.parquet\",  # Versioned path recommended\n",
    "    index=False,  # Don't persist index (redundant with ticker+date)\n",
    "    engine='pyarrow',  # More reliable than fastparquet\n",
    "    compression='gzip',  # ~60-70% size reduction\n",
    ")\n",
    "\n",
    "# Post-save verification\n",
    "saved_size = os.path.getsize(f\"{PROCESSED_DIR}/cleaned_stocks.parquet\")/1e6\n",
    "print(f\"âœ… Saved cleaned data ({saved_size:.1f}MB)\")\n",
    "print(f\"Columns persisted: {df_features.columns.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
