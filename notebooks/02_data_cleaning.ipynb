{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pandas_ta as ta  # Technical indicators\n",
    "from tqdm import tqdm   # Progress bars\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "RAW_DIR = \"../data/raw/yfinance\"\n",
    "PROCESSED_DIR = \"../data/processed\"\n",
    "print(\"Files in yfinance folder:\", os.listdir(RAW_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk Load All YFinance Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_yfinance():\n",
    "    \"\"\"Load and combine multiple YFinance stock data CSVs into one DataFrame.\n",
    "    Handles metadata rows, standardizes columns, and ensures date continuity.\"\"\"\n",
    "    \n",
    "    all_dfs = []  # Will store individual stock DataFrames before concatenation\n",
    "    \n",
    "    # Process each CSV file in directory (with progress bar)\n",
    "    for file in tqdm(os.listdir(RAW_DIR)):\n",
    "        if file.endswith('.csv'):\n",
    "            try:\n",
    "                # Extract ticker symbol from filename (e.g. 'AAPL.csv' -> 'AAPL')\n",
    "                ticker = file.split('.')[0]\n",
    "                \n",
    "                # Read CSV with specific formatting:\n",
    "                # - Skip 3 metadata rows\n",
    "                # - Standardize column names\n",
    "                # - Parse dates as datetime objects\n",
    "                df = pd.read_csv(\n",
    "                    f\"{RAW_DIR}/{file}\",\n",
    "                    skiprows=3,\n",
    "                    names=['date', 'close', 'high', 'low', 'open', 'volume'],\n",
    "                    parse_dates=['date']\n",
    "                )\n",
    "                \n",
    "                # Add ticker column to identify the stock\n",
    "                df['ticker'] = ticker\n",
    "                \n",
    "                # Ensure continuous daily data:\n",
    "                # 1. Set date as index\n",
    "                # 2. Force daily frequency (inserts NaNs for missing dates)\n",
    "                # 3. Forward-fill missing values\n",
    "                # 4. Reset index to return date to a column\n",
    "                df = df.set_index('date').asfreq('D').ffill().reset_index()\n",
    "                \n",
    "                all_dfs.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Skip problematic files but log the error\n",
    "                print(f\"⚠️ Failed {file}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    # Combine all DataFrames, ignoring original indices\n",
    "    return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Execute and verify load\n",
    "df = load_all_yfinance()\n",
    "print(f\"✅ Loaded {len(df)} rows from {df['ticker'].nunique()} stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code verification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['ticker'] == 'AAPL'].head(10))\n",
    "unique_tickers = df['ticker'].unique().tolist()\n",
    "print(f\"Tickers: {unique_tickers[:5]}...\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning Pipeline\n",
    "Goal: Fix common data quality issues\n",
    "\n",
    "Why?\n",
    "Ensures dates are recognized as timestamps (not strings)\n",
    ", Handles market closures without leaving gaps\n",
    ", Guarantees no NaN values break your models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Convert text dates to proper datetime format (essential for time series)\n",
    "    df['date'] = pd.to_datetime(df['date'])  \n",
    "    \n",
    "    # Forward-fill missing values (e.g., weekends/holidays when markets are closed)\n",
    "    df = df.sort_values(['ticker', 'date'])\n",
    "    df = df.groupby('ticker').apply(lambda x: x.ffill())  # Carry last known value forward\n",
    "    \n",
    "    # Remove any remaining bad rows\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total NaN values in entire DataFrame\n",
    "total_nans = df.isna().sum().sum()\n",
    "print(f\"Total NaN values: {total_nans}\")\n",
    "\n",
    "# NaN count per column\n",
    "nan_per_column = df.isna().sum()\n",
    "print(\"\\nNaN per column:\")\n",
    "print(nan_per_column)\n",
    "print(df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "Goal: Add technical indicators traders use\n",
    "\n",
    "Why These Indicators?\n",
    "\n",
    "SMA: Smooths price noise to reveal trends\n",
    "\n",
    "RSI: Identifies potential reversals (values >70 = overbought, <30 = oversold)\n",
    "\n",
    "MACD: Shows momentum shifts\n",
    "\n",
    "Bollinger Bands: Highlights volatility extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df):\n",
    "    \"\"\"Calculate technical indicators for each stock in the DataFrame.\n",
    "    \n",
    "    Applies common trading indicators to each stock's price series independently\n",
    "    to avoid cross-contamination between different securities.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing stock data with columns: ['ticker', 'date', 'close', ...]\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional technical indicator columns:\n",
    "        - sma_20: 20-day Simple Moving Average (trend identification)\n",
    "        - rsi_14: 14-day Relative Strength Index (momentum)\n",
    "        - macd: MACD line (12,26,9 EMAs crossover system)\n",
    "        - boll_high: Upper Bollinger Band (2 std dev above SMA)\n",
    "        - boll_low: Lower Bollinger Band (2 std dev below SMA)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group by ticker to ensure calculations are performed per-stock\n",
    "    # Using apply(lambda) pattern for clean column-wise operations\n",
    "    return df.groupby('ticker').apply(lambda x: x.assign(\n",
    "        # Trend indicator: 20-period Simple Moving Average\n",
    "        # Helps identify the prevailing market direction\n",
    "        sma_20=ta.sma(x['close'], 20),\n",
    "        \n",
    "        # Momentum oscillator: 14-period RSI\n",
    "        # Measures speed of price movements (range: 0-100)\n",
    "        # >70 = overbought, <30 = oversold\n",
    "        rsi_14=ta.rsi(x['close'], 14),\n",
    "        \n",
    "        # MACD Line (12,26,9 EMA configuration)\n",
    "        # Shows relationship between two moving averages\n",
    "        # Positive = upward momentum, Negative = downward momentum\n",
    "        macd=ta.macd(x['close'])['MACD_12_26_9'],\n",
    "        \n",
    "        # Bollinger Bands (20MA ± 2 standard deviations)\n",
    "        # Identifies overextended price moves\n",
    "        # Prices near upper band = potentially overbought\n",
    "        # Prices near lower band = potentially oversold\n",
    "        boll_high=ta.bbands(x['close'])['BBU_5_2.0'],\n",
    "        boll_low=ta.bbands(x['close'])['BBL_5_2.0']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = add_technical_indicators(df)\n",
    "# Check new columns\n",
    "print(\"New columns:\", df_features.columns.tolist())\n",
    "# See sample data for AAPL\n",
    "print(\"\\nAAPL data with indicators:\")\n",
    "print(df_features[df_features['ticker'] == 'AAPL'].tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward-fill missing values (carry last known observation forward)\n",
    "# NOTE: This maintains continuous time series but WON'T fill:\n",
    "# 1. Leading NaNs (where no prior value exists to fill from)\n",
    "# 2. Technical indicator warm-up periods (e.g., first 19 rows of SMA20)\n",
    "# For complete NaN removal, use .dropna() after this\n",
    "df_features = df_features.ffill()\n",
    "\n",
    "# drop early rows with NaN\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "print(\"Remaining NaNs:\", df_features.isna().sum().sum())  # Should be 0\n",
    "print(df_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any ticker has date gaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_gaps = df.groupby('ticker')['date'].apply(lambda x: x.sort_values().diff().max())\n",
    "print(\"Max gap between dates per ticker (should be <=3 days for weekends):\")\n",
    "print(date_gaps.dt.days.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicate dates per ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_dates = df.groupby(['ticker', 'date']).size().loc[lambda x: x > 1]\n",
    "if not duplicate_dates.empty:\n",
    "    print(\"⚠️ Duplicate dates found:\", duplicate_dates.index.tolist())\n",
    "\n",
    "# Verify volume > 0 for all rows\n",
    "zero_volume = df[df['volume'] <= 0]\n",
    "if not zero_volume.empty:\n",
    "    print(\"⚠️ Stocks with zero volume:\", zero_volume['ticker'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Tasks:\n",
    "# 1. Resolve data structure issues\n",
    "# 2. Validate data quality\n",
    "# 3. Calculate technical indicators\n",
    "# 4. Handle missing data appropriately\n",
    "\n",
    "# 1. CREATE CLEAN WORKING COPY\n",
    "# Always work on copies to prevent accidental mutation of source data\n",
    "# Note: This breaks reference chains that can cause SettingWithCopy warnings\n",
    "df_features = df_features.copy()\n",
    "\n",
    "# 2. FIX INDEX/COLUMN CONFLICTS\n",
    "# Common issue after groupby operations - ticker ends up in both index and columns\n",
    "if 'ticker' in df_features.index.names:\n",
    "    # Save the index values before resetting\n",
    "    ticker_values = df_features.index.get_level_values('ticker')\n",
    "    \n",
    "    # Clean slate - remove all index levels\n",
    "    # drop=True prevents old index from becoming new column\n",
    "    df_features = df_features.reset_index(drop=True)\n",
    "    \n",
    "    # Only add ticker back if it's not already a column\n",
    "    # Prevents duplicate columns which break pandas operations\n",
    "    if 'ticker' not in df_features.columns:\n",
    "        df_features['ticker'] = ticker_values\n",
    "\n",
    "# 3. DATA QUALITY CHECKS\n",
    "print(\"\\n🔍 Running Data Quality Checks...\")\n",
    "\n",
    "# Check 1: Negative Prices (impossible in real markets)\n",
    "# Important because some indicators break with negative values\n",
    "negative_prices = df_features[\n",
    "    (df_features['close'] <= 0) | \n",
    "    (df_features['open'] <= 0) |\n",
    "    (df_features['high'] <= 0) |\n",
    "    (df_features['low'] <= 0)\n",
    "]\n",
    "if not negative_prices.empty:\n",
    "    print(\"⚠️ Negative prices found in:\", negative_prices['ticker'].unique())\n",
    "    # Remove corrupt rows completely - can't fix bad prices\n",
    "    df_features = df_features[~df_features.index.isin(negative_prices.index)]\n",
    "\n",
    "# Check 2: Price Relationship Sanity\n",
    "# Ensures high > low, high >= close, etc. (market mechanics)\n",
    "invalid_prices = df_features[\n",
    "    (df_features['high'] < df_features['low']) |\n",
    "    (df_features['high'] < df_features['close']) |\n",
    "    (df_features['low'] > df_features['open'])\n",
    "]\n",
    "if not invalid_prices.empty:\n",
    "    print(\"⚠️ Illogical prices in:\", invalid_prices['ticker'].unique())\n",
    "    # Drop invalid rows - likely data corruption\n",
    "    df_features = df_features[~df_features.index.isin(invalid_prices.index)]\n",
    "\n",
    "# 4. FEATURE ENGINEERING (WITH NaN PROTECTION)\n",
    "print(\"\\n⚙️ Calculating Features with Safe NaN Handling...\")\n",
    "\n",
    "# Volume Analysis - helps detect unusual activity\n",
    "# Using min_periods=1 means we get values even with limited history\n",
    "df_features['volume_ma_20'] = df_features.groupby('ticker')['volume'].transform(\n",
    "    lambda x: x.rolling(20, min_periods=1).mean()\n",
    ")\n",
    "# Volume spike ratio (safe division)\n",
    "df_features['volume_spike'] = np.where(\n",
    "    df_features['volume_ma_20'] > 0,  # Prevent divide-by-zero\n",
    "    df_features['volume'] / df_features['volume_ma_20'],\n",
    "    1.0  # Neutral value when no history exists\n",
    ")\n",
    "\n",
    "# Technical Indicators Wrapper\n",
    "# Group-level calculation with error handling\n",
    "def safe_technical(group):\n",
    "    \"\"\"Calculate indicators for a single stock with error protection\"\"\"\n",
    "    try:\n",
    "        # Trend indicators\n",
    "        group['sma_20'] = ta.sma(group['close'], length=20)  # 20-day moving average\n",
    "        # Momentum indicators\n",
    "        group['rsi_14'] = ta.rsi(group['close'], length=14)  # Relative Strength Index\n",
    "        # Volatility indicator\n",
    "        group['atr_14'] = ta.atr(group['high'], group['low'], group['close'], length=14)\n",
    "        return group\n",
    "    except Exception as e:\n",
    "        # Log errors but keep processing other stocks\n",
    "        print(f\"⚠️ Error calculating indicators for {group.name}: {str(e)}\")\n",
    "        return group\n",
    "\n",
    "# Apply to all stocks in parallel\n",
    "df_features = df_features.groupby('ticker', group_keys=False).apply(safe_technical)\n",
    "\n",
    "# 5. NaN MANAGEMENT STRATEGY\n",
    "print(\"\\n🧹 Final NaN Cleanup...\")\n",
    "\n",
    "# Technical indicators create NaNs during warm-up periods:\n",
    "# - SMA20: First 19 days\n",
    "# - RSI14: First 13 days  \n",
    "# Strategy: Forward fill within each stock's data\n",
    "tech_cols = ['sma_20', 'rsi_14', 'atr_14']\n",
    "for col in tech_cols:\n",
    "    if col in df_features.columns:\n",
    "        # Fill NaNs with last valid observation per stock\n",
    "        df_features[col] = df_features.groupby('ticker')[col].ffill()\n",
    "\n",
    "# Final NaN treatment - domain-specific fallbacks\n",
    "fill_values = {\n",
    "    'sma_20': df_features['close'],  # If no SMA, use raw price\n",
    "    'rsi_14': 50,                   # Neutral RSI level\n",
    "    'atr_14': 0,                     # Assume no volatility\n",
    "    'volume_spike': 1                # No spike\n",
    "}\n",
    "for col, val in fill_values.items():\n",
    "    if col in df_features.columns:\n",
    "        df_features[col] = df_features[col].fillna(val)\n",
    "\n",
    "# 6. FINAL VALIDATION\n",
    "print(\"\\n✅ Final Validation\")\n",
    "# Hard checks - fail fast if data isn't clean\n",
    "assert df_features.isna().sum().sum() == 0, (\n",
    "    f\"Final NaN count: {df_features.isna().sum().sum()}\\n\"\n",
    "    f\"NaN columns: {df_features.columns[df_features.isna().any()].tolist()}\"\n",
    ")\n",
    "assert (df_features['close'] > 0).all(), \"Negative prices exist!\"\n",
    "\n",
    "# Ensure chronological order for time series analysis\n",
    "df_features.sort_values(['ticker', 'date'], inplace=True)\n",
    "\n",
    "print(f\"\\n🎉 Cleaning complete! Final shape: {df_features.shape}\")\n",
    "print(\"Available columns:\", df_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARKET DAY FLAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Creates boolean flag for trading vs non-trading days\n",
    "# Critical for: \n",
    "# - Avoiding false signals from weekend/holiday data\n",
    "# - Volume spike analysis\n",
    "# Note: Uses volume > 0 as proxy for market open (more reliable than calendar)\n",
    "df_features['is_market_open'] = df_features['volume'] > 0\n",
    "\n",
    "# 2. PRICE CHANGE ANALYSIS\n",
    "# ------------------------\n",
    "# Calculates daily returns with protection against:\n",
    "# - Division by zero\n",
    "# - Look-ahead bias\n",
    "# Groups by ticker to prevent cross-stock contamination\n",
    "df_features['daily_return'] = df_features.groupby('ticker')['close'].pct_change()\n",
    "\n",
    "# Extreme move detection (50% daily change)\n",
    "# Important for:\n",
    "# - Identifying potential data errors\n",
    "# - Flagging corporate actions (splits, dividends)\n",
    "# - Risk management scenarios\n",
    "extreme_returns = df_features[np.abs(df_features['daily_return']) > 0.5]\n",
    "if not extreme_returns.empty:\n",
    "    # Log but don't fail - some stocks legitimately gap\n",
    "    print(f\"⚠️ {len(extreme_returns)} extreme returns (>50%) detected\")\n",
    "    print(\"Sample affected tickers:\", extreme_returns['ticker'].unique()[:5])\n",
    "    \n",
    "\n",
    "\n",
    "print(df_features.head(10))\n",
    "\n",
    "# 3. DATA PERSISTENCE\n",
    "# -------------------\n",
    "# Saves cleaned data in Parquet format because:\n",
    "# - Preserves dtypes (no float→string conversion like CSV)\n",
    "# - Efficient columnar storage (faster for time series)\n",
    "# - Built-in compression (gzip offers good balance)\n",
    "# - Maintains schema on read\n",
    "df_features.to_parquet(\n",
    "    f\"{PROCESSED_DIR}/cleaned_stocks.parquet\",  # Versioned path recommended\n",
    "    index=False,  # Don't persist index (redundant with ticker+date)\n",
    "    engine='pyarrow',  # More reliable than fastparquet\n",
    "    compression='gzip',  # ~60-70% size reduction\n",
    ")\n",
    "\n",
    "# Post-save verification\n",
    "saved_size = os.path.getsize(f\"{PROCESSED_DIR}/cleaned_stocks.parquet\")/1e6\n",
    "print(f\"✅ Saved cleaned data ({saved_size:.1f}MB)\")\n",
    "print(f\"Columns persisted: {df_features.columns.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
